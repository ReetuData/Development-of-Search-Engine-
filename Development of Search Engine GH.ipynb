{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data preprocessing\n",
    "\n",
    "The data we have provided is not in processed form i.e not in a form suitable for developing a search engine.\n",
    "<br> The data has to be bought to a proper form so that the queries can efficiently match with the text in the documents for that certain steps need to be taken and for that you will need nltk library. (https://www.nltk.org/install.html)\n",
    "<br> Also your search engine should not treat words such as run, running, runnable as different words. For that you need to use stemming. (http://www.nltk.org/howto/stem.html)\n",
    "<br> You can also remove stopwords (Words which do not contribute much to the search such as \"and\",\"what\",etc which are very commonly used). (https://pythonprogramming.net/stop-words-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Building inverted index\n",
    "\n",
    "The next step in building a text search engine is assembling an inverted index. Let me explain what exactly that is. \n",
    "<br> An inverted index is a data structure that maps tokens to the documents they appear in. Every word in the query is referred to as a token. A word-level inverted index (or full inverted index or inverted list) contains the positions of each word within a document. Refer : https://en.wikipedia.org/wiki/Inverted_index\n",
    "\n",
    "The preprocessed text we have received in the previous stage here, we have to parse and tokenize (split into words). We need to remove the punctuations. Then create a dictionary which looks like this : \n",
    "\n",
    "{filename1: [word1,word2], filename2: [word3,word4]}\n",
    "\n",
    "Now the above definition of inverted index says that inverted index would map words to document names, but, we also want to support phrase queries: queries for not only words, but words in a specific sequence. For this, we’ll need to know where in each document each word shows up, so we can check for order. For each file we need the positions of the words.\n",
    "\n",
    "{word: [filename1, ...], ...}\n",
    "\n",
    "So our first task, then, is to create a mapping of words to their positions for each document, and then combine these to create our complete inverted index. This is what that looks like:\n",
    "word: {filename: [pos1, pos2]}\n",
    "\n",
    "For all words this is how it will look:\n",
    "\n",
    "{word: {filename1: [pos1, pos2]}, ...}, ...}\n",
    "\n",
    "And now, we have our index. We can input a word, and be returned a list of the documents it appears in, and the list of positions it appears in within these documents. Now, we’ll learn how to query this index.\n",
    "\n",
    "We will call the above dictionary : {word: {filename1: [pos1, pos2]}, ...}, ...} as \"invertedIndex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Querying the index (User will input a query and respective file should be displayed as output)\n",
    "\n",
    "Okay, so there are two types of queries we want to handle: standard queries, where at least one of the words in the query appears in the document, and phrase queries, where all the words in the query appear in the document in the same order.\n",
    "\n",
    "We advice you to implement standard queries first. A simple way to implement these is to split the query into words (tokens, as described above), get a list, for each word, which documents they appear in, and then union all of these lists. \n",
    "\n",
    "This code is pretty basic. All we’re doing here is sanitizing the input with a regular expression, and then returning a list of all the keys in the hashtable for that word in the index (which is just all the filenames that word appears in).\n",
    "\n",
    "In other words for each word in the query check with the keys in invertedIndex and then output the list of filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write you code here and use all the good coding concepts that you have learnt in the course and read on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case 1:\n",
    "\n",
    "**Input** :\n",
    "<br>Artificial linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "    <br>\n",
    "    <br>deep learning.txt\n",
    "        <br> **Matching words**: \n",
    "        <br> artificial\n",
    "        <br>linear\n",
    "        <br>\n",
    "        <br>linear regression.txt\n",
    "        <br> **Matching words**: \n",
    "        <br> artificial\n",
    "        <br>linear\n",
    "        <br> regression\n",
    "        <br>\n",
    "        <br>logistic regression.txt\n",
    "        <br> **Matching words**: \n",
    "        <br> artificial\n",
    "        <br>linear\n",
    "        <br> regression\n",
    "        <br>\n",
    "        <br> mean.txt\n",
    "        <br>**Matching words**: \n",
    "        <br> None\n",
    "        <br>\n",
    "        <br> standard deviation.txt\n",
    "        <br> **Matching words**: \n",
    "        <br> None\n",
    "        <br>\n",
    "        <br> variance.txt\n",
    "        <br> **Matching words**: \n",
    "        <br> linear\n",
    "        <br> regression\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a query is passed inside double quotes, it should be matched as it is. These queries are called as phrase queries.\n",
    "<br> Now to match a phrase query make sure all the words in the query appear in a document in the same order, if that happens, then output the file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case 2:\n",
    "\n",
    "**Input** :\n",
    "<br>\"intermediate representations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "    \n",
    "deep learning.txt\n",
    "        <br> **Matching words**: \n",
    "        <br> intermediate representations\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaka9003\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaka9003\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #there are many packages, download the one required here\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "filtered_sentence = []\n",
    "word_dict = {}\n",
    "def tokeniseData(txtdata, filename):\n",
    "    #print(file_name, txtdata)\n",
    "    tokenised_text = word_tokenize(txtdata)\n",
    "    file_list = []\n",
    "    for key, value in enumerate(tokenised_text):\n",
    "        indices = []\n",
    "        #print(\"w\", key, value)\n",
    "        if value not in stop_words:\n",
    "            indices.append(key)\n",
    "            stemmed_value = stemmer.stem(value)\n",
    "            #print(\"000\", stemmed_value, stemmed_value in word_dict)\n",
    "            if stemmed_value in word_dict:#\"key1\" in d\n",
    "                #print(\"true\")\n",
    "                word_dict[stemmed_value].update({filename: indices})\n",
    "            else:\n",
    "                #print(\"false\")\n",
    "                word_dict[stemmed_value] = {filename: indices}\n",
    "        \n",
    "    return word_dict\n",
    "\n",
    "\n",
    "\n",
    "import os.path\n",
    "file_content =[]\n",
    "word_dict = dict()\n",
    "dir = \"C:/Users/kaka9003/Documents/python_notes/milestone/files/\"\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith( \".txt\" ):\n",
    "        try:\n",
    "          with open( os.path.join( dir, file ) ,\"r\",encoding = \"ISO-8859-1\") as fd:\n",
    "            my_tokens = tokeniseData(fd.read(), file)\n",
    "            \n",
    "            #print(my_tokens)\n",
    "#             print(\"*\"*20)\n",
    "        except Exception as e:\n",
    "           print(\"error\",e)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"intermediate representations\"\n",
      "\u001b[95mFile:\u001b[96mdeep-learning.txt\n",
      "\u001b[95mMatching words:intermedi represent \n",
      "\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = str(input()) \n",
    "if(user_input[0]=='\"'):\n",
    "    flag = False\n",
    "    file = ''\n",
    "    matching_words = ''\n",
    "    user_input_stemmed = [stemmer.stem(i) for i in word_tokenize(user_input) if stemmer.stem(i).isalnum()]\n",
    "    compare_indices = 0\n",
    "    for val in user_input_stemmed:\n",
    "        if val in word_dict:\n",
    "            matching_words+=val+ \" \"\n",
    "            for value in word_dict[val]:\n",
    "                if(word_dict[val][value][0]-compare_indices>=0):\n",
    "                    compare_indices = word_dict[val][value][0];\n",
    "                    flag =  True\n",
    "                    file = value\n",
    "#                 print(color.PURPLE + \"File:\" + color.CYAN +  f)\n",
    "#                 print(color.PURPLE + \"Matching words:\")\n",
    "#                 print(val, end=\"\\n \")\n",
    "#                 print(\"\\n\")\n",
    "\n",
    "        else:\n",
    "            print(\"no\", val)\n",
    "    print(color.PURPLE + \"File:\" + color.CYAN +file)\n",
    "    print(color.PURPLE + \"Matching words:\"+matching_words)\n",
    "    print(\"\\n \")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "else:\n",
    "    user_input_stemmed = [stemmer.stem(i) for i in word_tokenize(user_input)]\n",
    "    #print(user_input_stemmed)\n",
    "    for val in user_input_stemmed:\n",
    "        #print(my_tokens[val])\n",
    "        if val in word_dict:\n",
    "            #print(\"yes\", val, word_dict[val])\n",
    "            for f in word_dict[val]:\n",
    "                print(color.PURPLE + \"File:\" + color.CYAN +  f)\n",
    "                print(color.PURPLE + \"Matching words:\")\n",
    "                print(val, end=\"\\n \")\n",
    "                print(\"\\n\")\n",
    "\n",
    "        else:\n",
    "            print(\"no\", val)\n",
    "\n",
    "    print(color.BOLD + 'Task Completed' + color.END)\n",
    "    #\"intermediate representations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial linear regression\n",
      "\u001b[95mFile:\u001b[96mdeep-learning.txt\n",
      "\u001b[95mMatching words:\n",
      "artifici\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlinear-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "artifici\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlogistic-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "artifici\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mdeep-learning.txt\n",
      "\u001b[95mMatching words:\n",
      "linear\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlinear-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "linear\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlogistic-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "linear\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mvariance.txt\n",
      "\u001b[95mMatching words:\n",
      "linear\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlinear-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "regress\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlogistic-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "regress\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mvariance.txt\n",
      "\u001b[95mMatching words:\n",
      "regress\n",
      " \n",
      "\n",
      "\u001b[1mTask Completed\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning \n",
      "\u001b[95mFile:\u001b[96mdeep-learning.txt\n",
      "\u001b[95mMatching words:\n",
      "deep\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mdeep-learning.txt\n",
      "\u001b[95mMatching words:\n",
      "learn\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlinear-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "learn\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mlogistic-regression.txt\n",
      "\u001b[95mMatching words:\n",
      "learn\n",
      " \n",
      "\n",
      "\u001b[95mFile:\u001b[96mvariance.txt\n",
      "\u001b[95mMatching words:\n",
      "learn\n",
      " \n",
      "\n",
      "\u001b[1mTask Completed\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
